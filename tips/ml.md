
- [基于趋势和季节性的时间序列预测](https://mp.weixin.qq.com/s/Ln4E9iZd3b3EZqeEjNNsag)
  - 时间序列模式
    - 时间序列预测模型使用数学方程(s)在一系列历史数据中找到模式。然后使用这些方程将数据[中的历史时间模式投射到未来。
      - 趋势:数据的长期增减。趋势可以是任何函数，如线性或指数，并可以随时间改变方向。
      - 季节性:以固定的频率(一天中的小时、星期、月、年等)在系列中重复的周期。季节模式存在一个固定的已知周期
      - 周期性:当数据涨跌时发生，但没有固定的频率和持续时间，例如由经济状况引起。
      - 噪音:系列中的随机变化。
    - 当季节波动不随时间序列水平变化时，加法分解是最合适的方法。相反，当季节成分的变化与时间序列水平成正比时，则采用乘法分解更为合适。
  - 分解数据
    - 从数学意义上讲，如果一个时间序列的均值和方差不变，且协方差与时间无关，那么这个时间序列就是平稳的。
    - 如何检验时间序列的平稳性呢?
      - 一方面，我们可以通过检查时间序列的均值和方差来手动检查。另一方面，我们可以使用测试函数来评估平稳性。
      - 查看趋势
        - ADF检验的结果(p值低于0.05)表明，存在的原假设可以在95%的置信水平上被拒绝。因此，如果p值低于0.05，则时间序列是平稳的
        - KPSS检验的结果(p值高于0.05)表明，在95%的置信水平下，不能拒绝的零假设。因此如果p值低于0.05，则时间序列不是平稳的。
        - 统计结果还显示了时间序列的平稳性的影响。虽然两个检验的零假设是相反的。ADF检验表明时间序列是平稳的(p值> 0.05)，而KPSS检验表明时间序列不是平稳的(p值> 0.05)。但这个数据集创建时带有轻微的趋势，因此结果表明，KPSS测试对于分析这个数据集更准确。
      - 检查季节性
        - 正如在之前从滑动窗口中观察到的，在我们的时间序列中有一个季节模式。因此应该采用差分方法来去除时间序列中潜在的季节或周期模式。由于样本数据集具有12个月的季节性，我使用了365个滞后差值:
      - 分解模式
        - 在看了分解图的四个部分后，可以说，在我们的时间序列中有很强的年度季节性成分，以及随时间推移的增加趋势模式
  - 时序建模
    - Autoregression (AR)
    - Moving Average (MA)
    - Autoregressive Moving Average (ARMA)
    - Autoregressive Integrated Moving Average (ARIMA)
    - Seasonal Autoregressive Integrated Moving-Average (SARIMA)
    - Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)
    - Vector Autoregression (VAR)
    - Vector Autoregression Moving-Average (VARMA)
    - Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)
    - Simple Exponential Smoothing (SES)
    - Holt Winter’s Exponential Smoothing (HWES)
  - 由于我们的数据中存在季节性，因此选择HWES，因为它适用于具有趋势和/或季节成分的时间序列数据。
  - 这种方法使用指数平滑来编码大量的过去的值，并使用它们来预测现在和未来的“典型”值。指数平滑指的是使用指数加权移动平均(EWMA)“平滑”一个时间序列。使用均方根误差(RMSE)作为评估模型误差的度量的实现。
- [AB 实验](https://mp.weixin.qq.com/s/2sE-KxdRAvnp3GBOBU4Cfg)
  - AB 实验需要注意️辛普森悖论、幸存者偏差、选择偏差等，注意事项都是来源于对撞因子，简单来说就是「是指同时被两个以上的变数影响的变数」
  - 如何衡量
    - 对于任何一个想法我们很难去衡量它的好坏，大胆假设小心求证。短期目标可能会与更关键的长期目标发生冲突。
    - 新奇效应如何避免？足够的样本量能保证一个合理的实验周期，可以使用我们的流量计算器中计算流量和实验周期，从而避免这种新奇效应的影响。
  - 架构
    - 流量分割 分流和分层
      - 每个独立实验为一层，层与层之间流量是正交的（简单来讲，就是一份流量穿越每层实验时，都会再次随机打散，且随机效果离散）。实验在同一层拆分流量，不论如何拆分，不同组的流量是不重叠的。
      - 分流是指我们直接将整体用户切割为几块，用户只能在一个实验中。但是这种情况很不现实，因为如果我要同时上线多个实验，流量不够切怎么办？那为了达到最小样本量，我们就得延长实验周期，要是做一个实验，要几个月。
        - 分流是指对流量进行整体切割，实验之间互斥。
        - 目的：为了获取纯净的分区，不会互相影响。
        - 缺点：浪费流量，导致流量不够。
      - 分层就是将同一批用户，不停的随机后，处于不同的桶。也就是说，一个用户会处于多个实验中，只要实验之间不相互影响，我们就能够无限次的切割用户。这样在保证了每个实验都能用全流量切割的同时，也保证了实验数据是置信的。
        - 目的：同一个用户在不同的实验组，相互不会影响。
        - 缺点：不同层之间的 hash 值尽量不要重合。
    - 随机算法
      - 按照密码学来将「随机」分为三种级别：1. 伪随机 (PRNG) 2. 密码学安全的伪随机 (CSPRNG) 3. 真随机 (TRNG)
  - 实验结果显著
    - 两类统计学错误
      - 在统计学的世界里，我们往往只说概率，不说确定，在现实世界中往往只能基于样本进行推断。在 AB 实验中，我们 不知道真实情况是什么，因此做假设检验的时候就会犯错误，这种错误可以划分为两类：
        - 这是第一类错误：实际没有区别，但实验结果表示有区别，我们得到显著结果因此否定原假设，认为实验组更优，发生的概率用 𝛂 表示。
        - 这是第二类错误：实际有区别，但是实际结果表示没有区别，我们得到不显著的结果因此无法拒绝原假设，认为实验组和对照组没有区别，发生的概率用 𝜷 表示。
  - ![img.png](ml_abtest.png)
- [ChatGPT如何获取的超能力](https://mp.weixin.qq.com/s/X5ZcCkuEVtrTz0lJnt5a7w)
  - ChatGPT有人类语言中的所有词（又称token），这是它的搜索空间。
  - 然后，精心选择高质量的文本数据（包括代码），训练Transformer模型，需要很多的GPU算力，进行大量的矩阵运算，达到预定的训练目标即可结束训练。这里，Transformer模型是一个包含所有token的概率模型或开放空间。
  - 然后再用含有人类反馈的强化学习（RLHF）来进一步调整Transformer模型来适应人类的价值观和使用规则。现在，Transformer模型被人类调教后的包含所有token的概率模型或限制空间。
  - 最后，执行任务的时候，就是给出一些提示tokens，或上下文context，在Transformer构成的所有token的限制空间中使用贪婪，集束，温度采用等策略来找到概率最大的可能的token的排列组合。这个组合，就是看到的ChatGPT的输出。
- [Prompt Engineering Guide](https://www.promptingguide.ai/techniques/knowledge)
- [Parameter optimization in neural networks](https://www.deeplearning.ai/ai-notes/optimization/index.html?_hsmi=218814757&utm_campaign=The%20Batch&utm_medium=email&utm_content=218804890&utm_source=hs_email&_hsenc=p2ANqtz-_FluhJbN2619klYO-hikBLp6-aEAP60t0VaLzoiEItfCyfrdJguDchLz7Q6h5imUeQp3SkfQaBZnlD8_aUcP5U97FiMA)


