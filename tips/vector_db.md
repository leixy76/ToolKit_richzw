
- [优化 Milvus 性能](https://mp.weixin.qq.com/s/4gDsAF4QnmXWzomrSFRLLg)
    - Milvus 是读写分离且无状态的向量数据库，状态信息储存在 etcd 中，coordinator 节点去 etcd 请求状态并修改状态
        - 当用户需要查看状态信息、清理状态信息场景时，etcd 调试工具必不可少。
        - [BirdWatcher  是 Milvus 2.0 项目的调试工具，该工具连接 etcd 并检查 Milvus 系统的某些状态](https://mp.weixin.qq.com/s/ot-eMCKqM7aP5pEbGaMIQA)
    - Milvus 单机
        - 在单机模式下，milvus内置一个rocksdb用于代替pulsar的功能，rdb_data目录里的东西是rockdb管理的，所有insert/delete/upsert的数据都会先在rocksdb里存一份做为write ahead log，然后querynode datanode从rocksdb里把数据拉出来消费
        - 如果rocksdb里的数据消费完了，不会立刻删除，因为rocksdb有自己的gc，只不过这些数据对milvus来说已经消费过了，放着只是为了保证数据安全性，一旦milvus崩了，再启动的时候，那些没被持久化到minio里的数据还能从rocksdb里拉回来
    - 合理的预计数据量，表数目大小，QPS 参数等指标
    - 选择合适的索引类型和参数
        - 索引的选择对于向量召回的性能至关重要，Milvus 支持了 Annoy，Faiss，HNSW，DiskANN 等多种不同的索引，用户可以根据对延迟、内存使用和召回率的需求进行选择
        - 是否需要精确结果？
            - 只有 Faiss 的 Flat 索引支持精确结果，但需要注意 Flat 索引检索速度很慢，查询性能通常比其他 Milvus 支持的索引类型低两个数量级以上，因此只适合千万级数据量的小查询
        - 数据量是否能加载进内存？
            - 对于大数据量，内存不足的场景，Milvus 提供两种解决方案：
                - DiskANN
                    - DiskANN 依赖高性能的磁盘索引，借助 NVMe 磁盘缓存全量数据，在内存中只存储了量化后的数据。
                    - DiskANN 适用于对于查询 Recall 要求较高，QPS 不高的场景。
        - 构建索引和内存资源是否充足
            - 性能优先，选择 HNSW 索引
- [Milvus 2.0 数据插入与持久化](https://mp.weixin.qq.com/s/D0xdD9mqDgxFvNY19hvDgQ)
    - 删数据逻辑
        - 调用delete删除一条已存在的数据时，它只是在某个segment里把某条数据标记为deleted，但是这个segment的数据此时仍然是一个整体，那条被删的数在内存里仍然占着空间。
        - 当你又调用insert增加一条数据时，这条新数据实际上是放入一个新的segment中，这条新数据也会占用额外内存空间。因此，随着你继续删除+insert，你会看到内存用量增加，新的这个segment执行的是暴搜，cpu用量会增加。
        - 随着新的segment中的数据达到一定量可以建索引了，indexnode就开始给这个新segment建索引，建索引就会消耗cpu。只有当某个segment中被删的数据达到20%以上，datanode开始对这个segment进行compact，
        - 把deleted的数据去除掉，剩下的数据存为一个新的segmemt。在compact之后有可能会发生小segment合并成大segment。总之，删除和更新数据会产生很多额外的工作，消耗内存消耗cpu，设计上就如此
    - 如果用num_enrtities观察行数的话，是看不出变化的，因为num_entities不统计被删除的行数。如果你是删除之后再用query去查询主键，还能查到的话，那八成是因为你是删完就立即query，而consistency_level没有设为Strong
- [动态 Schema](https://mp.weixin.qq.com/s/jhyePhxjUbWBicEvqxIKGQ)
  - Milvus 如何实现动态 Schema 功能
    - Milvus 通过用隐藏的元数据列的方式，来支持用户为每行数据添加不同名称和数据类型的动态字段的功能。
    - 当用户创建表并开启动态字段时，Milvus 会在表的 Schema 里创建一个名为$meta的隐藏列。JSON 是一种不依赖语言的数据格式，被现代编程语言广泛支持，因此 Milvus 隐藏的动态实际列使用 JSON 作为数据类型。
  - 动态 Schema 的 A、B 面
    - 一方面，动态 Schema 设置简便，无需复杂的配置即可开启动态 Schema；动态 Schema 可以随时适应数据模型的变化，开发者无需进行重构或调整代码。
    - 另一方面，使用动态 Schema 进行过滤搜索比固定 Schema 慢得多；在动态 Schema 上进行批量插入比较复杂，推荐用户使用行式插入接口写入动态字段数据。
- [向量数据库](https://developer.aliyun.com/article/1328709?spm=a2c6h.12883283.index.43.5ba74307ZagBs5)
    - 本质上就是给定一条向量，我们要搜索离它最近的 k 条向量，那最近的这个距离的定义可以是，比如说，内积或者欧氏距离、或者余弦距离。有了距离的定义以后，我们还要定义一些向量的搜索算法。
    - 向量搜索算法总体来说分为两类，
        - 第一类是精准搜索 KNN - FLAT，一个向量一个向量地去检索，然后取 top k，召回率会很高，但是它的执行的性能会比较差，因为要做全局扫描
        - approximate nearest neighbor，就是 ANN，那这类算法它可能回答的并不是最精准的 top k 的向量，但是这一类算法的好处是执行效率比较高。
- 海量数据相似数据查找方法
    - 高维稀疏向量和稠密向量两大方向
        - 高维稀疏向量的相似查找 - minhash, lsh(Locality-Sensitive Hashing）, simhash
            - minhash
                - 定义一个函数h：计算集合S最小的minhash值，就是在这种顺序下最先出现1的元素
                - 如果进行n次重排的话，就会有n个minhash函数，{h1(S), h2(S)…, hn(S)}, 那原来每个高维集合，就会被降到n维空间，比如S1->{h1(S1), h2(S1)…, hn(S1)}
                - 实际中因为重排比较耗时，会用若干随机哈希函数替代. 同样可以定义n个哈希函数【不需要重排，每个hash计算对应的值就行】，进行上述操作，那每个集合S就被降维到n维空间的签名。
            - LSH
                - minhash解决了高维向量间计算复杂度问题(通过minhash 机制把高维降低到n维低纬空间)
                - 但是还没解决一个问题：两两比较，时间复杂度O(n^2)
                - LSH 就是这样的机制，通过哈希机制，让相似向量尽可能出现一个桶中，而不相似的向量出现在不同的桶中. 相似度计算只在么个桶中进行，每个桶彼此之间不做相似度计算。
                - 在minhashing 签名的基础上做LSH
                    - 一个高维向量通过minhashing处理后变成n维低维向量的签名，现在把这n维签名分成b组，每组r个元素。
                    - 每组通过一个哈希函数，把这组的r个元素组成r维向量哈希到一个桶中。
                    - 每组可以使用同一个哈希函数，但是每组桶没交集，即使哈希值一样。桶名可以类似：组名+哈希值。
                    - 在一个桶中的向量才进行相似度计算，相似度计算的向量是minhash的n维向量（不是r维向量）。
            - simHash
                - Simhash技术引入到海量文本去重领域
                - google 通过Simhash把一篇文本映射成64bits的二进制串。
                    - 文档每个词有个权重。
                    - 文档每个词哈希成一个二进制串。
                    - 文档最终的签名是各个词和签名的加权和(如果该位是1则+weight，如果是0，则-weight)，再求签名[>0则变成1，反之变成0]得到一个64位二进制数。
                    - 如果两篇文档相同，则他们simhash签名汉明距离小于等于3。
                - 因为simhash本质上是局部敏感hash，所以可以使用海明距离来衡量simhash值的相似度。
                - 假设我们要寻找海明距离3以内的数值，根据抽屉原理，只要我们将整个64位的二进制串划分为4块，无论如何，匹配的两个simhash code之间至少有一块区域是完全相同的。
    - 高效的搜索算法有很多，其主要思想是通过两种方式提高搜索效率：
        - 减少向量大小——通过降维或减少表示向量值的长度。
        - 缩小搜索范围——可以通过聚类或将向量组织成基于树形、图形结构来实现，并限制搜索范围仅在最接近的簇中进行，或者通过最相似的分支进行过滤。
    - ANN 最近邻检索
        - [Comprehensive Guide To Approximate Nearest Neighbors Algorithms](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)
        - 树方法，如 KD-tree，Ball-tree，Annoy
        - 哈希方法，如 Local Sensitive Hashing (LSH)
        - 矢量量化方法，如 Product Quantization (PQ)
        - 近邻图方法，如 Hierarchical Navigable Small World (HNSW)
    - 近似最近邻 (ANN)算法
      - - [ANN]
  - [Comprehensive Guide To Approximate Nearest Neighbors Algorithms](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)
        - 「LSH」（Locality-Sensitive Hashing）」它引入了一种哈希函数，使得相似的输入能以更高的概率映射到相同的桶中，其中桶的数量远小于输入的数量。
        - 「ANNOY（Approximate Nearest Neighbors）」它的核心数据结构是随机投影树，实际是一组二叉树，其中每个非叶子节点表示一个将输入空间分成两半的超平面，每个叶子节点存储一个数据。二叉树是独立且随机构建的，因此在某种程度上，它模仿了哈希函数。ANNOY会在所有树中迭代地搜索最接近查询的那一半，然后不断聚合结果。这个想法与 KD 树非常相关，但更具可扩展性。
        - 「HNSW（Hierarchical Navigable Small World）」它受到小世界网络思想的启发，其中大多数节点可以在很少的步骤内被任何其他节点到触达；例如社交网络的“六度分隔”理论。
            - HNSW构建这些小世界图的层次结构，其中底层结构包含实际数据。中间的层创建快捷方式以加快搜索速度。执行搜索时，HNSW从顶层的随机节点开始，导航至目标。当它无法靠近时，它会向下移动到下一层，直到到达最底层。上层中的每个移动都可能覆盖数据空间中的很长一段距离，而下层中的每个移动都可以细化搜索质量。
        - 「FAISS（facebook AI Similarity Search）」它运行的假设是：高维空间中节点之间的距离服从高斯分布，因此这些数据点之间存在着聚类点。faiss通过将向量空间划分为簇，然后在簇内使用用向量量化。faiss首先使用粗粒度量化方法来查找候选簇，然后进一步使用更精细的量化方法来查找每个簇。
            - [上手Faiss](https://mp.weixin.qq.com/s/GxxPqa1pjDvt9PvAMuebkA)
        - 「ScaNN（Scalable Nearest Neighbors）」的主要创新在于各向异性向量量化。它将数据点量化为一个向量，使得它们的内积与原始距离尽可能相似，而不是选择最接近的量化质心点。
            - [blog](https://blog.research.google/2020/07/announcing-scann-efficient-vector.html) [Vector Search ANN 服务](https://cloud.google.com/vertex-ai/docs/matching-engine/ann-service-overview?hl=zh-cn)
            - ScaNN 算法的核心思想是使用一种称为 Product Quantization（PQ）的技术将高维向量压缩成多个低维向量，并使用这些低维向量进行相似性搜索。PQ 技术可以将高维向量划分成多个子向量，并对每个子向量进行量化，从而将高维向量压缩成多个低维向量。这样可以大大降低相似性搜索的计算复杂度，提高搜索效率。
            - ScaNN 算法还使用了一种称为 Clustering Graph（CG）的技术，将数据集划分成多个子集，并构建一个图来表示这些子集之间的相似性关系。这样可以将相似的向量聚集在一起，从而提高搜索效率。
            - ScaNN 算法的优点在于它可以在大规模数据集上进行高效的相似性搜索，同时具有较高的搜索准确率。ScaNN 算法还支持增量式更新，可以动态地添加和删除向量，从而适应数据集的变化。
    - Faiss 
        - 我们可以将向量想象为包含在 Voronoi 单元格中 - 当引入一个新的查询向量时，首先测量其与质心 (centroids) 之间的距离，然后将搜索范围限制在该质心所在的单元格内。
        - 为了解决搜索时可能存在的遗漏问题，可以将搜索范围动态调整，例如当 nprobe = 1 时，只搜索最近的一个聚类中心，当 nprobe = 2 时，搜索最近的两个聚类中心，根据实际业务的需求调整 nprobe 的值。
    - Product Quantization (PQ)
        - 在大规模数据集中，聚类算法最大的问题在于内存占用太大
            - 保存每个向量的坐标，而每个坐标都是一个浮点数，占用的内存就已经非常大了。
            - 还需要维护聚类中心和每个向量的聚类中心索引，这也会占用大量的内存。
        - 对于第一个问题，可以通过量化 (Quantization) 的方式解决，也就是常见的有损压缩.例如在内存中可以将聚类中心里面每一个向量都用聚类中心的向量来表示，并维护一个所有向量到聚类中心的码本，这样就能大大减少内存的占用。
            - 但是在高维坐标系中，还会遇到维度灾难问题，具体来说，随着维度的增加，数据点之间的距离会呈指数级增长，这也就意味着，在高维坐标系中，需要更多的聚类中心点将数据点分成更小的簇，才能提高分类的质量。否者，向量和自己的聚类中心距离很远，会极大的降低搜索的速度和质量。
        - 对于第二个问题，将向量分解为多个子向量，然后对每个子向量独立进行量化，比如将 128 维的向量分为 8 个 16 维的向量，然后在 8 个 16 维的子向量上分别进行聚类，因为 16 维的子向量大概只需要 256 个聚类中心就能得到还不错的量化结果，所以就可以将码本的大小从 2^64 降低到 8 * 256 = 2048 个聚类中心，从而降低内存开销
    - Hierarchical Navigable Small Worlds (HNSW) 类似 skiplist
        - 这种方法的基本思想是每次将向量加到数据库中的时候，就先找到与它最相邻的向量，然后将它们连接起来，这样就构成了一个图。当需要搜索的时候，就可以从图中的某个节点开始，不断的进行最相邻搜索和最短路径计算，直到找到最相似的向量。
        - HNSW 继承了相同的分层格式，最高层具有更长的边缘（用于快速搜索），而较低层具有较短的边缘（用于准确搜索）
    - 相似性测量 (Similarity Measurement)
        - 欧几里得距离（Euclidean Distance）
            - 欧几里得距离算法的优点是可以反映向量的绝对距离，适用于需要考虑向量长度的相似性计算。
            - 例如推荐系统中，需要根据用户的历史行为来推荐相似的商品，这时就需要考虑用户的历史行为的数量，而不仅仅是用户的历史行为的相似度
        - 余弦相似度（Cosine Similarity）
            - 余弦相似度是指两个向量之间的夹角余弦值
            - 余弦相似度算法的优点是可以反映向量的方向，适用于不需要考虑向量长度的相似性计算。因此适用于高维向量的相似性计算。例如语义搜索和文档分类。
        - 点积相似度 (Dot product Similarity)
            - 点积相似度是指两个向量的点积，也就是两个向量对应位置的元素相乘之后再求和。点积相似度算法的优点是可以反映向量的绝对距离和方向，适用于需要考虑向量长度的相似性计算。例如推荐系统中，需要根据用户的历史行为来推荐相似的商品，这时就需要考虑用户的历史行为的数量，而不仅仅是用户的历史行为的相似度。
            - 点积相似度算法的缺点是需要对向量进行归一化，否则会受到向量长度的影响。例如在推荐系统中，如果用户的历史行为数量很多，那么用户的历史行为向量的长度就会很大，这样就会导致点积相似度算法的结果偏向于历史行为数量较少的用户。
            - 点积相似度算法的优点在于它简单易懂，计算速度快，并且兼顾了向量的长度和方向。它适用于许多实际场景，例如图像识别、语义搜索和文档分类等。但点积相似度算法对向量的长度敏感，因此在计算高维向量的相似性时可能会出现问题。
    - 过滤 (Filtering)
        - 在实际的业务场景中，往往不需要在整个向量数据库中进行相似性搜索，而是通过部分的业务字段进行过滤再进行查询。所以存储在数据库的向量往往还需要包含元数据，例如用户 ID、文档 ID 等信息。这样就可以在搜索的时候，根据元数据来过滤搜索结果，从而得到最终的结果。
        - 为此，向量数据库通常维护两个索引：一个是向量索引，另一个是元数据索引。然后，在进行相似性搜索本身之前或之后执行元数据过滤，但无论哪种情况下，都存在导致查询过程变慢的困难。
        - Pre-filtering：在向量搜索之前进行元数据过滤。虽然这可以帮助减少搜索空间，但也可能导致系统忽略与元数据筛选标准不匹配的相关结果。
        - Post-filtering：在向量搜索完成后进行元数据过滤。这可以确保考虑所有相关结果，在搜索完成后将不相关的结果进行筛选。
    - https://guangzhengli.com/blog/zh/vector-database/
- [向量检索技术](https://mp.weixin.qq.com/s/705usxBUmLcZL3rrB4Du9A)
  - 定义
    - 向量检索主要是做一个 K Nearest Neighbors (K最近邻，简称 KNN) 计算，目标是在N个D维的向量的库中找最相似的k个结果
    - KNN 计算通常代价比较大，很难在较短时间内返回结果，此外，在很多场景，用户并不需要绝对精确的相似结果
    - 通常会使用相似最近邻搜索，即 ANN 的方式来替代 KNN，从k个绝对最近似结果变成k个近似最优结果，以牺牲一定准确度的前提，得到更短的响应时间。
  - 向量检索的四种算法
    - Table-based，典型算法如 LSH
    - Tree-based，是把向量根据相似度去构造成一个树的结构
    - Cluster-based，也称为 IVF（Inverted File），把向量先进行聚类处理，检索时首先计算出最近的 k 个聚类中心，再在这些聚类中心中计算出最近的 k 个向量
    - Graph-based， 把向量按照相似度构建成一个图结构，检索变成一个图遍历的过程。常用算法是HNSW
  - ByteHouse
- [向量数据库](https://mp.weixin.qq.com/s/UCgJi7MfAnn8tAPvL3sldQ)
  - 向量检索算法
    - 基于树的方法，例如KDTree和Annoy
    - 基于图的方法，例如HNSW
    - 基于乘积量化的方法，例如SQ和PQ
    - 基于哈希的方法，例如LSH
    - 基于倒排索引的方法
    - 基于聚类 IVF
  - 数据压缩方式建立索引, 主要包括平坦压缩和量化压缩
    - 平坦压缩是指以未经修改的形式存储向量的索引，
    - 量化中索引的底层向量被分解成由较少字节组成的块（通常通过将浮点数转换为整数）以减少内存消耗和搜索过程中的计算成本。
  - Milvus
    - mmap功能在collection.load的时候会把s3上的数据文件下载到本地硬盘，所以首先本地硬盘得有同等大小。然后内存还是要的，最好能达到数据量的1/4以上。
    - 然后如果你用的是hnsw索引，性能可能会比纯内存慢一到两倍这样。如果用的ivf索引，性能会比纯内存慢一到两个数量级
- [Milvus]
  - 查询节点内存自动均衡的几种策略？当前默认是scorebase
  -  milvus的过滤做法是先按条件里的标量过一遍，把符合条件的条目标为1，不符合的标为0，然后做ANN搜索，碰到1的就计算距离，碰到0的就忽略。过滤的性能跟索引有关系，HNSW索引如果标为1的数量很少，就很慢。IVF索引不受这个影响，比较快
  - 物理文件删除？这个需要数据文件（segment） 上的数据都失效才能删掉。而segment失效，要么是上面的数据全被delete，要么是被compact。
    - 不要期望物理文件在删除后立即缩小。被删除的量要达到一定量才行，比如某个segment里被删除的数据达到了10%以上，才会触发一个动作把这10%的数据真正地从磁盘上清除。
    - 清除的流程是：用那90%的数据构建一个新的segment，然后把旧的segment标记为删除，等待垃圾清理机制做最终清除。
  - search返回的结果里不带有partition信息。可以建表时用一个字段来存partition的名字或者标记，然后search的时候在output_fields里填写这个字段的名字。
  - qps
    - qps受影响的因素很多，数据量，维度，索引类型参数，搜索参数，是否有过滤，是否有output_fields，milvus. yaml里面的queryNode.group里的配置，querynode数量，load的参数replica_number，等等。
    - 要获得更高的qps可以从上面这些方面入手。cpu的核数和性能也会影响qps，甚至NUMA架构也会影响qps。单机版的indexnode datanode如果有建索引或者compaction的任务在执行，也会影响qps。
  - Knowhere 是 Milvus 的内部核心引擎，负责向量搜索，是基于行业标准开源库（如 Faiss、DiskANN 和 hnswlib 等）的增强版本
    -  Knowhere 属于开源，其部署环境更多样，可在所有主机类型上运行
    - Knowhere 依赖于 OSS 库（如 Faiss、DiskANN 和 hnswlib）
  - [Cardinal 搜索引擎](https://mp.weixin.qq.com/s/4xx2U8Xyr1RetTkMtRrxyw)
    - Cardinal 是用现代 C++ 语言和实用的近似最近邻搜索（ANNS）算法构建的多线程、高效率向量搜索引擎
    - 同时能够处理暴搜请求和 ANNS 索引修改请求；处理各种数据格式，包括 FP32、FP16 和 BF16;执行索引 Top-K 和索引范围搜索（Range Search）;使用内存中数据或提供基于内存、磁盘和 MMap 等不同方式的索引
    - Cardinal 利用 x86 的 AVX-512 扩展和 ARM 的 NEON 及 SVE 指令集等尖端技术，提供针对高效计算优化的代码
    - 它引入了 AUTOINDEX 机制，自动选择适合于数据集最佳的搜索策略和索引。开发者无需手动调优，能够节省时间和精力
    - 内部算法
      - 搜索算法，包括基于 IVF 和基于图的方法 
      - 帮助搜索保持所需召回率的算法，不论过滤样本的百分比如
      - 更高效的 Best-First 搜索算法迭代方
      - 定制了优先队列数据结构中的算法
  - 估算容量
    - 5百万128维，原始数据量大约是2.5g，工具估算时会乘以一个安全系数，这个系数一般是2到3之间，所以你看到的Loading Memory是5G多点
    - 工具是按cluster估的，每个节点都给了推荐，如果不算etcd/minio/plasar这些的话，milvus的节点的推荐内存配置大约总共27. 5g  etcd推荐3*4g，minio推荐2*8g，pulsar的比较多，因为它本身也是个分布式系统 所以如果500万128维的向量其实必要用cluster，一个standalone就好了
  - 全量查询功能 - Query iterator
  - Milvus 2.3
    - Cosine 相似度类型： 无需向量归一化，简化数据搜索流程。
    - Upsert 数据：提升更新和删除数据的管理流程效率，适用于频繁更新数据且追求数据一致性和原子性的场景。
    - 范围搜索（Range Search）: 通过限制查询向量与其他向量之间的距离，范围搜索能够实现对搜索结果的有效细化，适用于搭建推荐引擎的场景。 
    - 支持 Parquet 文件：提升数据处理能力，支持 Parquet 文件，通过其高效的列式存储格式，提供更好的查询性能，适用于具有复杂数据集的场景。 
    - 支持 Array 数据类型：支持在搜索过程中基于多个属性进行精确的元数据过滤。在电商领域中，该功能支持根据不同产品标签进行搜索，为用户返回相关的搜索结果
- [BigANN 2023](https://mp.weixin.qq.com/s/7H7xtGzEfAdu-zQv0NHYzg)
  - Filters 赛道: 本赛道使用了 YFCC 100M 数据集，要求参赛者处理从该数据集中选取的 1000 万张图片
    - 具体任务要求为提取每张图片的特征并使用 CLIP 生成 Embedding 向量，且需包含图像描述、相机型号、拍摄年份和国家等元素的标签（元素均来自于词汇表）。
    - 主要挑战是参赛者需要成功地将 10 万个查询与数据集中的相应图像和标签匹配，每个查询都包含一个图像向量和特定的标签。
    - 解决方案是基于图算法和 tag 分类
      - Build 时，会对每种可能出现的 tag 组合进行 cardinality 分析；对于较包含向量较多的组合建图，其他的组合建立倒排索引；搜索时则根据不同的组合选取对应的搜索方式
      - 对 query 按所属的 tag 进行分类，搜索时分别对每个 tag 所对应的 query 依次进行搜索
        - 一是可以最大化 cache 利用率，二是在爆搜时可以使用矩阵乘法计算进行加速。当然，为了加速计算，我们使用了对数据进行量化，并利用 SIMD 优化相关的距离计算
  - Out-Of-Distribution(OOD) 赛道: 本赛道使用了 Yandex Text-to-Image 10M 数据集，强调跨模态数据的整合。
    - 基础数据集包括来自 Yandex 视觉搜索数据库的 1000 万张图像 Embedding 向量，这些向量由 Se-ResNext-101 模型生成的，但查询向量是基于文本搜索和不同的模型生成的
    - 参赛者需要有效地弥合这些不同模态数据之间的差异
    - 解决方案基于图算法以及高度优化的搜索过程
      - 计算方面，用不同精度的量化进行搜索和 refine，并使用 SIMD 进行计算加速
      - 搜索开始前，先对 query 向量进行聚类。在图搜时，对分属不同的聚类的 query 分配不同的初始点，并且对每个聚类进行依次搜索。
        - 聚类可以体现两大优势，一是对不同聚类依次搜索优化了 cache 利用率，二是由于对不同聚类分配自适应的初始点一定程度上缓解了向量不同分布的问题
        - 使用了多级的 bitset 数据结构
          - 在图搜的过程中，一般需要一个数据结构来标记哪些点被访问过，传统的方法是使用一个 bitset 或者哈希表，bitset 的缺点是其中大部分的内存并不会被使用到，使得每次读取时会有大概率的 cache miss
          - 哈希表的缺点是糟糕的常数导致性能低下
          - 受内存中多级页表的启发，设计出了多级 bitset 的数据结构，将上层 bitset 缓存在 cpu cache 中，使得读写性能得到大幅提升。
  - Sparse 赛道: 本赛道以 MSMARCO passage retrieval 数据集为核心，处理大量的文章（超过 880 万个），所有文章都通过 SPLADE 模型编码成稀疏向量。
    - 查询数量近 7000个，且查询也通过相同的模型处理，但长度较短、非零元素较少。
    - 本赛道的主要挑战是选手需准确检索给定查询的 Top 结果，重点是查询向量和数据库向量之间的最大内积。
    - 解决方案是基于图算法和一些基于稀疏向量的优化
      - 每个稀疏向量通过 (data[float32], index[int32]) 元组的列表进行表示，对 data 进行多精度量化，分别用户图搜时的计算和最后的 refine。同时 index 也可以用 int16 表示，以降低内存带宽使用。
      - 内积计算有一个重要特性即绝对值较大的值有更大的重要性，而绝对值较小的值有较小的重要性。
      - 可以根据这一特性设计剪枝策略，即在图搜过程中将绝对值较小的值裁剪掉，并在图搜结束后使用完整向量进行 refine。
      - 还用 SIMD 进行快速的有序列表求交集，进而实现高效的稀疏向量内积计算
  - Streaming 赛道: 本赛道基于 MS Turing 数据集中的一部分，包含 1000 万个数据
    - 参与者的任务是遵循提供的“操作手册”（该手册详细说明了一系列数据插入、删除和搜索操作），并在 1 小时内完成操作。此外，选手们还有 8GB DRAM 的限制
    - 赛道的重点是优化处理这些操作的过程，并保持数据集的索引流畅。
    - 解决方案是基于图算法以及 SQ 量化
      - 对于删除操作， 采用了惰性删除策略
      - 搜索中，对向量进行不同精度的量化分别用以图搜和 refine
        - 第一步是使用低精度量化后的向量进行图搜。
        - 由于使用惰性删除策略，搜索得到的一些向量已经被删除了，因此在第二步使用后置过滤的策略将被删除的向量过滤掉
        - 用更高精度的量化向量进行 refine 得到最终结果。
  - https://github.com/harsha-simhadri/big-ann-benchmarks/pulls?q=Zilliz+Solution+author%3Ahhy3
- [向量检索大赛](https://mp.weixin.qq.com/s/pngwV1Ibe4rxmtWrcr_D2g)
- Zilliz
  - Unstructured Data Meetup
    - [Feb](https://www.youtube.com/watch?v=42wZa3NasoM)























